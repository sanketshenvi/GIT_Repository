{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data= pd.read_csv(\"../input/emojify/emojify_data.csv\",usecols=[0,1],names=['colA', 'colB'])\n",
    "text=data[\"colA\"].tolist()\n",
    "emojis=np.array(data[\"colB\"])\n",
    "print(\"{} +++> {}\".format(text[1],emojis[1]))\n",
    "maxLen = len(max(text, key=len).split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = len(max(text, key=len).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "emoji_dictionary = {\"0\": \"\\u2764\\uFE0F\",\n",
    "                    \"1\": \":baseball:\",\n",
    "                    \"2\": \":smile:\",\n",
    "                    \"3\": \":disappointed:\",\n",
    "                    \"4\": \":fork_and_knife:\"}\n",
    "def label_to_emoji(label):\n",
    "    return emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C=len(emoji_dictionary)\n",
    "np.asarray(emojis)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False,categories='auto')\n",
    "emojis = emojis.reshape(len(emojis), 1)\n",
    "Y = onehot_encoder.fit_transform(emojis)\n",
    "print(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        \n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1   \n",
    "    return words_to_index,index_to_words, word_to_vec_map\n",
    "words_to_index,index_to_words,word_to_vec_map=read_glove_vecs(\"../input/glove6b50dtxt/glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index,max_len):\n",
    "    m = X.shape[0] \n",
    "    X_indices = np.zeros((m,max_len))\n",
    "    for i in range(m):\n",
    "        sentence_words =[words.lower() for words in X[i].split()]\n",
    "        \n",
    "        j=0\n",
    "        for w in sentence_words:\n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            j += 1\n",
    "    return X_indices\n",
    "X1 = np.array(text)\n",
    "X=sentences_to_indices(X1, words_to_index,maxLen)\n",
    "print(X)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.layers.embeddings import Embedding\n",
    "def pretrained_embedding_layer(word_to_vec_map, words_to_index):\n",
    "    vocab_len=len(words_to_index)+1\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0] \n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    for word, index in words_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    return embedding_layer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "def Emojify_V2(input_shape, word_to_vec_map, words_to_index):\n",
    "    sentence_indices = Input(input_shape, dtype='int32')\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, words_to_index)\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = LSTM(128, return_sequences=False)(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(5)(X)\n",
    "    X = Activation('softmax')(X)\n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "    return model\n",
    "model = Emojify_V2((maxLen,), word_to_vec_map, words_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.callbacks\n",
    "from keras.callbacks import TensorBoard\n",
    "filename=\"logger.csv\"\n",
    "checkpoint=keras.callbacks.CSVLogger(filename, separator=',', append=False)\n",
    "model.fit(X, Y,epochs = 50, batch_size = 32, shuffle=True,callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp=input(\"Enter the text: \")\n",
    "inputs=np.array([inp])\n",
    "X=sentences_to_indices(inputs, words_to_index,maxLen)\n",
    "print(\"{}:: {}\".format(inp,label_to_emoji(np.argmax(model.predict(X)))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
